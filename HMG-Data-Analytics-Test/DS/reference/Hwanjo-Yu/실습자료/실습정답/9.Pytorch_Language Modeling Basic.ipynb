{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9.Pytorch_Language Modeling Basic.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"A4PI4Cd-xBBC"},"source":["# Language Modeling with RNNs\n","[참고자료1](https://wikidocs.net/21668), \n","[참고자료2](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/main.py#L30-L50)"]},{"cell_type":"markdown","metadata":{"id":"19E_czi9xRS0"},"source":["Language Modeling (LM)은 언어라는 현상을 모델링하고자 단어 시퀀스 (또는 문장)에 확률을 할당한는 모델이다.\n","\n","다시 말하면, 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다. 단어 시퀀스에 확률을 할당하게 하기 위해 가장 보편적으로 사용되는 방법은 언어 모델이 **이전 단어들이 주어졌을 때 다음 단어를 예측** 하도록 하는 것이다."]},{"cell_type":"markdown","metadata":{"id":"gH2uwOIVyRMd"},"source":["**단어 시퀀스의 확률**\n","\n","하나의 단어를 $w$, 단어 시퀀스를 대문자 $W$ 라고 한다면, $n$개의 단어가 등장하는 단어 시퀀스 $W$의 확률은 다음과 같다.\n","\n","$$P(W)=P(w_1, w_2, \\cdots, w_n)$$\n","\n","**다음 단어 등장 확률**\n","\n","다음 단어가 등장할 확률을 식으로 표현하면, $n-1$개의 단어가 나열된 상태에서 $n$번째 단어의 확률은 다음과 같다.\n","\n","$$p(w_n|w_1, \\cdots, w_{n-1})$$\n","\n","전체 단어 시퀀스 $W$의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 다음과 같다. (Chain Rule)\n","$$P(W)=P(w_1, w_2, \\cdots, w_n)=\\prod_{i=1}^{n}P(w_i|w_1, \\cdots, w_{i-1})$$\n","\n","예제로 검색엔진에서 특정 단어를 치면 뒤에 단어를 제안 해주는 것을 생각 할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"GN-lHofkS8hi"},"source":["**Perplexity**\n","\n","Language Model의 성능을 평가하는 방법\n","\n","$$PPL(W)=P(w_1, w_2, \\cdots, w_N)^{-\\frac{1}{N}}$$\n","\n","PPL은 언어모델이 `헷갈리는 정도` 라고 해석 할 수 있다. 값이 낮을수록 좋은 성능이다.\n","\n","예를들어서 PPL이 10인 언어 모델은 모델이 테스트 데이터에 대해서 다음 단어를 예측 하는 모든 시점마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있다.\n","\n","$$PPL(W)=P(w_1, w_2, \\cdots, w_N)^{-\\frac{1}{N}}=(\\frac{1}{10}^{N})^{-\\frac{1}{N}}=10$$"]},{"cell_type":"markdown","source":["**Cross Entropy와의 관계**\n","\n","$$PPL=exp(Cross Entropy)$$"],"metadata":{"id":"7tTPdxH3Mm8s"}},{"cell_type":"markdown","metadata":{"id":"-WQdLL9ZxHrn"},"source":["## Requirements"]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/gdrive/')\n","\n","import os\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRXLmEK6U6h-","executionInfo":{"status":"ok","timestamp":1650590804977,"user_tz":-540,"elapsed":30349,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}},"outputId":"708153d0-ceac-43b8-d07f-190335b436ed"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","metadata":{"id":"qJZZ1nYzdRbD","executionInfo":{"status":"ok","timestamp":1650591365817,"user_tz":-540,"elapsed":5751,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["# Some part of the code was referenced from below.\n","# https://github.com/pytorch/examples/tree/master/word_language_model \n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import clip_grad_norm_"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_yXiG1UN5cHF"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"8eZEoKG5dTTh","executionInfo":{"status":"ok","timestamp":1650591453439,"user_tz":-540,"elapsed":438,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["import os\n","\n","# Dictionary\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","    \n","    def add_word(self, word):\n","        if not word in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","    \n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","# Corpus\n","class Corpus(object):\n","    def __init__(self):\n","        self.dictionary = Dictionary()\n","\n","    def get_data(self, path, batch_size=20):\n","        # Add words to the dictionary\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                tokens += len(words)\n","                for word in words: \n","                    self.dictionary.add_word(word)  \n","        \n","        # Tokenize the file content\n","        ids = torch.LongTensor(tokens)\n","        token = 0\n","        with open(path, 'r') as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    ids[token] = self.dictionary.word2idx[word]\n","                    token += 1\n","        num_batches = ids.size(0) // batch_size\n","        ids = ids[:num_batches*batch_size]\n","        # word index들의 sequence를 return\n","        return ids.view(batch_size, -1)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z54v8M1C5gil"},"source":["## Hyperparameters"]},{"cell_type":"markdown","source":["Train data download:\n","https://url.kr/sulxnh\n","<!-- https://drive.google.com/file/d/1vQRyXr5pdJfdAlzR7WUuOFoqJL4CRqK3/view?usp=sharing -->\n","\n","데이터의 경로는 `/gdrive/My Drive/Colab Notebooks/train.txt`로 지정해주시기 바랍니다."],"metadata":{"id":"-l1uMzKRefX3"}},{"cell_type":"code","metadata":{"id":"Eu2eAaSKdjgg","executionInfo":{"status":"ok","timestamp":1650591573390,"user_tz":-540,"elapsed":3662,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","embed_size = 128\n","hidden_size = 1024\n","\n","num_layers = 1\n","num_epochs = 5\n","\n","num_samples = 1000     # number of words to be sampled\n","\n","batch_size = 20\n","seq_length = 30\n","learning_rate = 0.002\n","\n","# Load \"Penn Treebank\" dataset\n","corpus = Corpus()\n","ids = corpus.get_data('./train.txt', batch_size)\n","vocab_size = len(corpus.dictionary)\n","num_batches = ids.size(1) // seq_length"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tth2qEvNh477","executionInfo":{"status":"ok","timestamp":1650591573391,"user_tz":-540,"elapsed":5,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}},"outputId":"5b8d8166-5843-429a-d290-5b92169ae872"},"source":["ids"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   0,    1,    2,  ...,  152, 4955, 4150],\n","        [  93,  718,  590,  ...,  170, 6784,  133],\n","        [  27,  930,   42,  ...,  392, 4864,   26],\n","        ...,\n","        [ 997,   42,  507,  ...,  682, 6849, 6344],\n","        [ 392, 5518, 3034,  ..., 2264,   42, 3401],\n","        [4210,  467, 1496,  ..., 9999,  119, 1143]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"C-FgZsWu5jde"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"wYePnZpdds3w","executionInfo":{"status":"ok","timestamp":1650592776878,"user_tz":-540,"elapsed":330,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["# RNN based language model\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(RNNLM, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, x, h):\n","        # Embed word ids to vectors\n","        x = self.embed(x)\n","        \n","        # Forward propagate LSTM\n","        out, (h, c) = self.lstm(x, h)\n","        \n","        # Reshape output to (batch_size*sequence_length, hidden_size)\n","        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n","        \n","        # Decode hidden states of all time steps\n","        out = self.linear(out)\n","        return out, (h, c)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b394wMaeUS-","executionInfo":{"status":"ok","timestamp":1650592788361,"user_tz":-540,"elapsed":9611,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nDH-7MSeVx3","executionInfo":{"status":"ok","timestamp":1650592841834,"user_tz":-540,"elapsed":313,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}}},"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Truncated backpropagation\n","def detach(states):\n","    return [state.detach() for state in states]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTImZuq3eY0-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650593088664,"user_tz":-540,"elapsed":189287,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}},"outputId":"fb9d2518-868f-4da9-93e7-f404d04e0b25"},"source":["# Train the model\n","for epoch in range(num_epochs):\n","    # Set initial hidden and cell states\n","    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n","              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n","    \n","    for i in range(0, ids.size(1) - seq_length, seq_length):\n","        # Get mini-batch inputs and targets\n","        inputs = ids[:, i:i+seq_length].to(device)\n","        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n","        \n","        # Forward pass\n","        states = detach(states)\n","        outputs, states = model(inputs, states)\n","        loss = criterion(outputs, targets.reshape(-1))\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        step = (i+1) // seq_length\n","        if step % 100 == 0:\n","            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n","                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Step[0/1549], Loss: 9.2100, Perplexity: 9996.64\n","Epoch [1/5], Step[100/1549], Loss: 6.0105, Perplexity: 407.68\n","Epoch [1/5], Step[200/1549], Loss: 5.9293, Perplexity: 375.87\n","Epoch [1/5], Step[300/1549], Loss: 5.7887, Perplexity: 326.58\n","Epoch [1/5], Step[400/1549], Loss: 5.6932, Perplexity: 296.84\n","Epoch [1/5], Step[500/1549], Loss: 5.1307, Perplexity: 169.13\n","Epoch [1/5], Step[600/1549], Loss: 5.2007, Perplexity: 181.40\n","Epoch [1/5], Step[700/1549], Loss: 5.3565, Perplexity: 211.99\n","Epoch [1/5], Step[800/1549], Loss: 5.2024, Perplexity: 181.72\n","Epoch [1/5], Step[900/1549], Loss: 5.0531, Perplexity: 156.50\n","Epoch [1/5], Step[1000/1549], Loss: 5.0896, Perplexity: 162.32\n","Epoch [1/5], Step[1100/1549], Loss: 5.3215, Perplexity: 204.69\n","Epoch [1/5], Step[1200/1549], Loss: 5.1978, Perplexity: 180.88\n","Epoch [1/5], Step[1300/1549], Loss: 5.0800, Perplexity: 160.77\n","Epoch [1/5], Step[1400/1549], Loss: 4.8208, Perplexity: 124.06\n","Epoch [1/5], Step[1500/1549], Loss: 5.1573, Perplexity: 173.69\n","Epoch [2/5], Step[0/1549], Loss: 5.4314, Perplexity: 228.46\n","Epoch [2/5], Step[100/1549], Loss: 4.5772, Perplexity: 97.24\n","Epoch [2/5], Step[200/1549], Loss: 4.6753, Perplexity: 107.27\n","Epoch [2/5], Step[300/1549], Loss: 4.6768, Perplexity: 107.43\n","Epoch [2/5], Step[400/1549], Loss: 4.5770, Perplexity: 97.22\n","Epoch [2/5], Step[500/1549], Loss: 4.1210, Perplexity: 61.62\n","Epoch [2/5], Step[600/1549], Loss: 4.4780, Perplexity: 88.05\n","Epoch [2/5], Step[700/1549], Loss: 4.3759, Perplexity: 79.51\n","Epoch [2/5], Step[800/1549], Loss: 4.4351, Perplexity: 84.36\n","Epoch [2/5], Step[900/1549], Loss: 4.1614, Perplexity: 64.16\n","Epoch [2/5], Step[1000/1549], Loss: 4.2342, Perplexity: 69.00\n","Epoch [2/5], Step[1100/1549], Loss: 4.5009, Perplexity: 90.10\n","Epoch [2/5], Step[1200/1549], Loss: 4.4617, Perplexity: 86.64\n","Epoch [2/5], Step[1300/1549], Loss: 4.2157, Perplexity: 67.74\n","Epoch [2/5], Step[1400/1549], Loss: 3.9492, Perplexity: 51.89\n","Epoch [2/5], Step[1500/1549], Loss: 4.3548, Perplexity: 77.85\n","Epoch [3/5], Step[0/1549], Loss: 4.4502, Perplexity: 85.64\n","Epoch [3/5], Step[100/1549], Loss: 3.8497, Perplexity: 46.98\n","Epoch [3/5], Step[200/1549], Loss: 4.0082, Perplexity: 55.05\n","Epoch [3/5], Step[300/1549], Loss: 4.0025, Perplexity: 54.73\n","Epoch [3/5], Step[400/1549], Loss: 3.9178, Perplexity: 50.29\n","Epoch [3/5], Step[500/1549], Loss: 3.4446, Perplexity: 31.33\n","Epoch [3/5], Step[600/1549], Loss: 3.8391, Perplexity: 46.49\n","Epoch [3/5], Step[700/1549], Loss: 3.6734, Perplexity: 39.39\n","Epoch [3/5], Step[800/1549], Loss: 3.8062, Perplexity: 44.98\n","Epoch [3/5], Step[900/1549], Loss: 3.4509, Perplexity: 31.53\n","Epoch [3/5], Step[1000/1549], Loss: 3.5860, Perplexity: 36.09\n","Epoch [3/5], Step[1100/1549], Loss: 3.7602, Perplexity: 42.96\n","Epoch [3/5], Step[1200/1549], Loss: 3.7777, Perplexity: 43.71\n","Epoch [3/5], Step[1300/1549], Loss: 3.4919, Perplexity: 32.85\n","Epoch [3/5], Step[1400/1549], Loss: 3.2687, Perplexity: 26.28\n","Epoch [3/5], Step[1500/1549], Loss: 3.6091, Perplexity: 36.93\n","Epoch [4/5], Step[0/1549], Loss: 3.9160, Perplexity: 50.20\n","Epoch [4/5], Step[100/1549], Loss: 3.2397, Perplexity: 25.53\n","Epoch [4/5], Step[200/1549], Loss: 3.5148, Perplexity: 33.61\n","Epoch [4/5], Step[300/1549], Loss: 3.4292, Perplexity: 30.85\n","Epoch [4/5], Step[400/1549], Loss: 3.3202, Perplexity: 27.66\n","Epoch [4/5], Step[500/1549], Loss: 2.8632, Perplexity: 17.52\n","Epoch [4/5], Step[600/1549], Loss: 3.4263, Perplexity: 30.76\n","Epoch [4/5], Step[700/1549], Loss: 3.1563, Perplexity: 23.48\n","Epoch [4/5], Step[800/1549], Loss: 3.3158, Perplexity: 27.54\n","Epoch [4/5], Step[900/1549], Loss: 2.9756, Perplexity: 19.60\n","Epoch [4/5], Step[1000/1549], Loss: 3.1221, Perplexity: 22.70\n","Epoch [4/5], Step[1100/1549], Loss: 3.1669, Perplexity: 23.73\n","Epoch [4/5], Step[1200/1549], Loss: 3.2867, Perplexity: 26.76\n","Epoch [4/5], Step[1300/1549], Loss: 2.9947, Perplexity: 19.98\n","Epoch [4/5], Step[1400/1549], Loss: 2.7661, Perplexity: 15.90\n","Epoch [4/5], Step[1500/1549], Loss: 3.1485, Perplexity: 23.30\n","Epoch [5/5], Step[0/1549], Loss: 4.0401, Perplexity: 56.83\n","Epoch [5/5], Step[100/1549], Loss: 2.8488, Perplexity: 17.27\n","Epoch [5/5], Step[200/1549], Loss: 3.1277, Perplexity: 22.82\n","Epoch [5/5], Step[300/1549], Loss: 3.0896, Perplexity: 21.97\n","Epoch [5/5], Step[400/1549], Loss: 2.9841, Perplexity: 19.77\n","Epoch [5/5], Step[500/1549], Loss: 2.5968, Perplexity: 13.42\n","Epoch [5/5], Step[600/1549], Loss: 3.0769, Perplexity: 21.69\n","Epoch [5/5], Step[700/1549], Loss: 2.8237, Perplexity: 16.84\n","Epoch [5/5], Step[800/1549], Loss: 3.0980, Perplexity: 22.15\n","Epoch [5/5], Step[900/1549], Loss: 2.6371, Perplexity: 13.97\n","Epoch [5/5], Step[1000/1549], Loss: 2.8187, Perplexity: 16.76\n","Epoch [5/5], Step[1100/1549], Loss: 2.8025, Perplexity: 16.49\n","Epoch [5/5], Step[1200/1549], Loss: 2.9916, Perplexity: 19.92\n","Epoch [5/5], Step[1300/1549], Loss: 2.6762, Perplexity: 14.53\n","Epoch [5/5], Step[1400/1549], Loss: 2.4302, Perplexity: 11.36\n","Epoch [5/5], Step[1500/1549], Loss: 2.8605, Perplexity: 17.47\n"]}]},{"cell_type":"code","metadata":{"id":"CG4s6q_zeaoH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650593671471,"user_tz":-540,"elapsed":1368,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}},"outputId":"b2a132a7-5900-458b-93b2-0ce7397f53db"},"source":["# Test the model\n","with torch.no_grad():\n","    with open('sample.txt', 'w') as f:\n","        # Set intial hidden ane cell states\n","        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n","                 torch.zeros(num_layers, 1, hidden_size).to(device))\n","\n","        # Select one word id randomly\n","        prob = torch.ones(vocab_size)\n","        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n","\n","        for i in range(num_samples):\n","            # Forward propagate RNN \n","            output, state = model(input, state)\n","\n","            # Sample a word id\n","            prob = output.exp()\n","            word_id = torch.multinomial(prob, num_samples=1).item()\n","\n","            # Fill input with sampled word id for the next time step\n","            input.fill_(word_id)\n","\n","            # File write\n","            word = corpus.dictionary.idx2word[word_id]\n","            word = '\\n' if word == '<eos>' else word + ' '\n","            f.write(word)\n","\n","            if (i+1) % 100 == 0:\n","                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled [100/1000] words and save to sample.txt\n","Sampled [200/1000] words and save to sample.txt\n","Sampled [300/1000] words and save to sample.txt\n","Sampled [400/1000] words and save to sample.txt\n","Sampled [500/1000] words and save to sample.txt\n","Sampled [600/1000] words and save to sample.txt\n","Sampled [700/1000] words and save to sample.txt\n","Sampled [800/1000] words and save to sample.txt\n","Sampled [900/1000] words and save to sample.txt\n","Sampled [1000/1000] words and save to sample.txt\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfd5Z9SrwseB","executionInfo":{"status":"ok","timestamp":1650367679985,"user_tz":-540,"elapsed":11,"user":{"displayName":"JaeHyung Lee","userId":"18392552705726331072"}},"outputId":"ea38c3e8-2047-4b3a-d0b4-5c244a527067"},"source":["i"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["999"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[""],"metadata":{"id":"UcFEpoaWYgj2"},"execution_count":null,"outputs":[]}]}